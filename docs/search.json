[
  {
    "objectID": "accessibility/index.html",
    "href": "accessibility/index.html",
    "title": "Accessibility Statement",
    "section": "",
    "text": "Approach\nI am committed to ensuring accessibility for all visitors to my website. I strive to provide an inclusive and user-friendly experience for everyone, regardless of their abilities or disabilities.\n\n\nStandards and Guidelines\nI have designed and developed this site with the aim of meeting accessibility standards and guidelines. I follow the Web Content Accessibility Guidelines (WCAG) 2.1, a globally recognized standard for web accessibility. By adhering to these guidelines, I aim to make my website perceivable, operable, understandable, and robust for all users.\n\n\nAccessible Design\nI have taken several measures to ensure an accessible design, including using clear and easy-to-read fonts, providing sufficient color contrast for readability, and using alt text for images to give descriptions. I have also structured the content intending to make it easier for screen readers and assistive technologies to interpret and navigate the site.\n\n\nKeyboard navigation and assistive technology\nI have made efforts to ensure that my website is keyboard accessible, allowing you to navigate and interact with the content without relying solely on a mouse. I have also tested the site with popular screen readers and assistive technologies to ensure compatibility and improve accessibility for those of you who are visually impaired.\n\n\nContinuous Improvement\nAccessibility is an ongoing process, and I am committed to continuously improving the accessibility of my website. I regularly review and update the site to address accessibility issues and incorporate feedback.\n\n\nContact Me\nI’m aware of the limitations of my perspective and warmly welcome any suggestions for improvement. So don’t hesitate to contact me. Thank you for visiting my website, and I appreciate your support in promoting accessibility for everyone.\nI adapted this page from Tamara Sredojevic.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "John Baker",
    "section": "",
    "text": "MIT License\nCopyright © 2025 John Richard Baker Jr.\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”) to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright and permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS” WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n Back to top"
  },
  {
    "objectID": "work/2021/07/02/biodiversity/index.html",
    "href": "work/2021/07/02/biodiversity/index.html",
    "title": "Biodiversity in National Parks",
    "section": "",
    "text": "Photo of Great Smoky Mountains National Park by Robert Thiemann on Unsplash\nOn Earth, there are an estimated 8.7 million living species. Each is linked to another, producing a complicated web of life. As a result, approximately 100 extinctions per million species occur each year; exotic and invasive species replace native species, disrupt natural systems, and decrease life’s variety. In addition, climate change, diseases, land conversion and fragmentation, and other stressors exacerbate biodiversity decline.\nTo explore this, I interpreted biodiversity data from the National Park Service, particularly around the various species observed in different national park locations. I attempted to view my work from the perspective of a biodiversity analyst for the National Park Service. The National Park Service wants to ensure the survival of at-risk species and maintain biodiversity within their parks. Therefore, my main objectives were to understand species’ characteristics, conservation status, and those species’ relationship to the national parks. The following are some questions I posed:\nYou can find the data behind this project here.\nNote: The data was extracted on June 20, 2021.\nThe project can be viewed on GitHub."
  },
  {
    "objectID": "work/2021/07/02/biodiversity/index.html#preparation",
    "href": "work/2021/07/02/biodiversity/index.html#preparation",
    "title": "Biodiversity in National Parks",
    "section": "Preparation",
    "text": "Preparation\nI utilized Jupyter Notebooks in Visual Studio Code for this project. In addition, I used the Python libraries pandas, NumPy, Matplotlib, and seaborn to aid in my data preparation, analysis, and visualization. Once I loaded the data into a data frame, I took a top-level look at its contents.\n\n\n\nThe first five lines of the dataset\n\n\nAs I first looked at the dataset, I wondered what the column title T&E meant. I had to refer to the source to learn that it was shorthand for “Threatened and Endangered.” To make things more straightforward, I changed the name of the column to Conservation Status.\n\n\n\nI changed the name of column T&E to Conservation Status.\n\n\nNext, I discovered that there are 35,716 rows and six columns in the data frame. As I explored the data more in-depth, I decided to find the number of distinct species present: 33,628 — WOW! Then, I wanted to see the number of classified living creatures represented in the data. There are 14, including animals and plants. After that, I tried to drill down to see the size of each Category. Insects have the largest share of species with 14,053 category entries, and reptiles are the smallest at 99.\nAnother column I wanted to explore is Conservation Status. It has five categories: SC, T, DM, E, and nan (not a number) values. (Here, nan denotes that these species don’t have conservation status.) Instead of leaving the values ambiguous, I decided to recheck the source for their proper terms and update them accordingly.\n\n\n\nThe updated Conservation Statuses\n\n\nNext, I wanted to view the breakdown of the categories in the Conservation Status column. There are 35,090 nan values, 597 species of concern, 13 endangered species, seven threatened, and 9 in recovery. The last tasks I performed before initiating my proper analysis were to check the number of parks and species observations in the dataset. There are only four parks, and there are 306,120 total sightings logged in the data set; that’s a lot of viewings!"
  },
  {
    "objectID": "work/2021/07/02/biodiversity/index.html#analysis",
    "href": "work/2021/07/02/biodiversity/index.html#analysis",
    "title": "Biodiversity in National Parks",
    "section": "Analysis",
    "text": "Analysis\nTo begin analyzing the data, I first cleaned and explored the Conservation Status column. It has a few possible values:\n\nSpecies of Concern: declining or appear to need conservation\nThreatened: vulnerable to endangerment soon\nEndangered: seriously at risk of extinction\nIn Recovery: currently neither in danger of extinction throughout all or a significant portion of its range.\n\nDuring my initial exploration, I noticed many nan values, which were not desirable, so I converted them to be No Intervention. Next, I examined the different categories nested in the Conservation Status column, except those that do not require an intervention.\nFor those with the Endangered status, five are mammals, and four are birds. However, there are six birds and three mammals with the In Recovery status, which could mean the birds have bounced back more than the mammals.\n\n\n\nBirds may be recovering from endangerment more quickly than mammals.\n\n\nThat would be encouraging news since there are more birds in the Species of Concern category than any other variety.\n\n\n\nSince there are more birds in the Species of Concern category, it would be encouraging if they’re recovering from endangerment.\n\n\nNext, I wondered if certain species are more likely to be endangered. To answer this question, I first created a new column dubbed Is Protected, which included any species with a value other than No Intervention. Then, I grouped the column by Category and whether that category is protected or not. From the table below, it’s easy to see that birds, vascular plants, and mammals have a higher number of species protected.\n\n\n\nIt’s easy to see that birds, vascular plants, and mammals have a higher number of species protected.\n\n\nPlain-Jane numbers are not always the most useful statistic. Therefore, it’s also crucial to calculate the rate of protection that each category exhibits in the data. For example, approximately 19 percent of mammals and 39 percent of birds were under guard from the following table.\n\n\n\nApproximately 19 percent of mammals and 39 percent of birds were under guard.\n\n\n\nStatistical Significance\nI ran a couple of chi-squared tests to see if different species have statistically significant differences in conservation status rates. To start, I created a contingency table.\nFor the first test, I examined the difference between mammals and birds. The result indicated that there doesn’t seem to be a significant relationship between mammals and birds, so the next pair I tested the difference between was reptiles and mammals. According to the chi-squared test I ran, the difference between reptiles and mammals is statistically significant; mammals have a significantly higher rate of needed protection than reptiles.\n\n\nSpecies in Parks\nThe next set of analyses come from species sighting data. First, I looked at species’ Common Names to get an idea of the most prevalent animals in the dataset. Next, I split the data up into individual names. Then, I cleaned up duplicate words in each row to not count more than once per species. Next, I collapsed the words into one list for easier use. After that, to make the remainder of my analysis less complicated, I set all No Intervention values under the Observations column to 0 and the number type to float.\nAt this point, the data was cleaned up enough to count the occurrence of each word. As such, I determined that the phrase Bat occurred the most.\n\n\n\nBat occurs more than any other word counted.\n\n\nThere are several different scientific names for other bats in the data, so I wanted to figure out which rows of species referred to bats. To that end, I created a new column made up of boolean values to check if is_bat is True.\n\n\n\nDoes the Common Name refer to a species of bat?\n\n\nGreat Smoky Mountains National Park observed the most bats, while Bryce Canyon National Park detected none.\n\n\n\nGreat Smoky Mountains National Park observed the most bats, while Bryce Canyon National Park detected none.\n\n\nI wanted to see each park broken down by protected bat sightings vs. non-protected bat sightings. Except for Bryce Canyon National Park, every place has more sightings of protected bats — a good sign!\n\n\n\nEvery park has more sightings of protected bats than non-protected bats.\n\n\nAccording to the graph below, there are more sightings of protected bats than non-protected species. The national parks are doing a great job protecting their bat populations!\n\n\n\nThe national parks are doing a great job protecting their bat populations!"
  },
  {
    "objectID": "work/2021/07/02/biodiversity/index.html#conclusion",
    "href": "work/2021/07/02/biodiversity/index.html#conclusion",
    "title": "Biodiversity in National Parks",
    "section": "Conclusion",
    "text": "Conclusion\nThis project makes several inferences about the various species in four of the National Parks that comprise this data set. I was also able to answer the questions first posed in the beginning:\n\nThe vast majority of species were not part of the conservation effort (33,473 vs. 377).\nMammals and birds are the most protected species.\nWhile mammals and birds did not significantly differ in conservation percentage, mammals and reptiles did.\nBats were sighted the most, and they were most likely to be found at Great Smoky Mountains National Park.\n\nThe National Park Service is staunchly involved in the preservation of a variety of environments. National parks are becoming vital biodiversity reserves in the wake of growing global threats. However, we need help to improve and expand biodiversity conservation efforts.\nAccording to the National Park Service, approximately 80 to 90% of living organisms in our national parks remain unknown. Biodiversity discovery activities often require only excited minds and willing hands, not expertise. Children and non-scientists alike can help with conservation efforts. In addition, parks can develop new, engaging relationships with diverse audiences to discover life on our lands.\nPublic involvement and education can yield an exponential increase in awareness of and motivation for stewardship of biodiversity. By working together, we can encourage a public who understands how biodiversity supports the fabric of our lives. We can begin to seed this understanding into our cultural DNA and how we engage with the living world through that knowledge."
  },
  {
    "objectID": "work/2024/06/26/enrollment/index.html",
    "href": "work/2024/06/26/enrollment/index.html",
    "title": "Projecting Elementary and Secondary School Enrollment",
    "section": "",
    "text": "According to the National Center for Education Statistics (NCES), total public and private elementary and secondary school enrollment was 56 million in fall 2019, representing a three percent increase since fall 2010. However, between fall 2019 and the first fall of the coronavirus pandemic in 2020, enrollment decreased two percent. From fall 2020 to fall 2030, enrollments are expected to decrease another six percent. Both public and private school enrollments are projected to be lower in 2030 than in 2019.\n\n\n\n\n\n\n\n\n\nAccurate enrollment projections in elementary and secondary institutions are crucial for several reasons:\n\nBudgeting and resource allocation: Enrollment projections help school districts plan their budgets effectively. By estimating the number of students expected to attend, districts can allocate resources appropriately, ensuring they have enough teachers, classrooms, and educational materials to meet the demand.\nShort-term and long-term planning: Enrollment projections inform both short-term and long-term decision-making. In the short term, they help determine staffing needs and budgeting for specific programs. In the long term, they assist in planning for capital projects, such as building new schools or expanding existing facilities.\nPublic trust and support: Accurate enrollment projections can demonstrate the need for capital projects to the public, potentially influencing the outcome of school bond referenda. When the community understands the necessity of these projects based on reliable data, they are more likely to support them.\n\nAccurate enrollment projections are vital for effective budgeting, resource allocation, short-term and long-term planning, maintaining public trust, ensuring financial stability, and informed decision-making in elementary and secondary institutions. With that said, many institutions lack a reliable means by which to make long-range enrollment forecasts. I’m here to help."
  },
  {
    "objectID": "work/2024/06/26/enrollment/index.html#introduction",
    "href": "work/2024/06/26/enrollment/index.html#introduction",
    "title": "Projecting Elementary and Secondary School Enrollment",
    "section": "",
    "text": "According to the National Center for Education Statistics (NCES), total public and private elementary and secondary school enrollment was 56 million in fall 2019, representing a three percent increase since fall 2010. However, between fall 2019 and the first fall of the coronavirus pandemic in 2020, enrollment decreased two percent. From fall 2020 to fall 2030, enrollments are expected to decrease another six percent. Both public and private school enrollments are projected to be lower in 2030 than in 2019.\n\n\n\n\n\n\n\n\n\nAccurate enrollment projections in elementary and secondary institutions are crucial for several reasons:\n\nBudgeting and resource allocation: Enrollment projections help school districts plan their budgets effectively. By estimating the number of students expected to attend, districts can allocate resources appropriately, ensuring they have enough teachers, classrooms, and educational materials to meet the demand.\nShort-term and long-term planning: Enrollment projections inform both short-term and long-term decision-making. In the short term, they help determine staffing needs and budgeting for specific programs. In the long term, they assist in planning for capital projects, such as building new schools or expanding existing facilities.\nPublic trust and support: Accurate enrollment projections can demonstrate the need for capital projects to the public, potentially influencing the outcome of school bond referenda. When the community understands the necessity of these projects based on reliable data, they are more likely to support them.\n\nAccurate enrollment projections are vital for effective budgeting, resource allocation, short-term and long-term planning, maintaining public trust, ensuring financial stability, and informed decision-making in elementary and secondary institutions. With that said, many institutions lack a reliable means by which to make long-range enrollment forecasts. I’m here to help."
  },
  {
    "objectID": "work/2024/06/26/enrollment/index.html#how-to-project-enrollment",
    "href": "work/2024/06/26/enrollment/index.html#how-to-project-enrollment",
    "title": "Projecting Elementary and Secondary School Enrollment",
    "section": "How to Project Enrollment",
    "text": "How to Project Enrollment\nFor this project, I attempted to emulate the work of the NCES, specifically their Projections of Education Statistics to 2030   (Irwin et al. 2024).\n\n\n\n\n\n\nNoteClick here for a detailed methodology for projecting student enrollment using various statistical techniques.\n\n\n\n\n\n\nProjection Techniques\nThere are several key methods that can be used for projecting student enrollment in elementary and secondary institutions such as ratio-based methods, regression-based methods, the dwelling unit multiplier method, and the extended demographic model. After some research and experimentation, I elected to use an exponential smoothing technique.\n\nExponential Smoothing\nSingle exponential smoothing is a forecasting method suited for data that is relatively stable over time, where future values are expected to be around the same central value as observed historically, without significant shifts up or down. In developing projections of elementary and secondary enrollments, for example, the rate at which students progress from one particular grade to the next (e.g., from grade 2 to grade 3) can be projected using single exponential smoothing. Thus, this percentage is assumed to be constant over the forecast period.\nGenerally, exponential smoothing places more weight on recent observations than on earlier ones. The weights for observations decrease exponentially as one moves further into the past. As a result, the older data have less influence on the projections. The rate at which the weights of older observations decrease is determined by the smoothing constant.\nWhen using single exponential smoothing for a time series, \\(P_t\\), a smoothed series, \\(P\\), is computed recursively by evaluating where \\[\\hat{P}_t = \\alpha~P_t + (1 - \\alpha) P_{t-1}\\]\\(0 &lt; \\alpha \\leq 1\\) is the smoothing constant.\nBy repeated substitution, we can rewrite the equation as \\[\nP_t = \\alpha \\sum_{s=0}^{t-1} (1 - \\alpha)^s P_{t-s}\\]where time, \\(s\\), goes from the first period in the time series, \\(0\\), to time period \\(t-1\\). The forecasts are constant for all years in the forecast period. The constant equals \\[\\hat{P}_{T+k} = \\hat{P}_t\\]where \\(t\\) is the last year of actual data and \\(k\\) is the \\(k^{th}\\) year in the forecast period where \\(k &gt; 0\\).\nThese equations illustrate that the projection is a weighted average based on exponentially decreasing weights. For higher smoothing constants, weights for earlier observations decrease more rapidly than for lower smoothing constants.\n\n\n\nApproach Overview\nI utilized the grade progression rate method to project grades 2 through 12. With this approach, a rate of progression from each grade (1 through 11) to the next grade (2 through 12) was projected using single exponential smoothing. For example, the rate of progression from grade 2 to grade 3 is the current year’s grade 3 enrollment expressed as a percentage of the previous year’s grade 2 enrollment. To calculate enrollment for each year in the forecast period, the progression rate for each grade was applied to the previous year’s enrollment in the previous grade.\nI also utilized the enrollment rate method to project prekindergarten, kindergarten, and first-grade enrollments as well as elementary and secondary ungraded enrollments. In this method, an enrollment rate for each grade (or ungraded level) was projected using single exponential smoothing. For example, the enrollment rate for grade 1 is the number of students enrolled in grade 1 divided by the number of 6-year-old children. To calculate enrollment for each year in the forecast period, the enrollment rate for each category was applied to the projected population in the appropriate age group.\n\nAssumptions Underlying This Approach\nThe grade progression rate method assumes that past trends affecting public and private elementary and secondary school enrollments will continue over the forecast period. This assumption implies that all factors influencing enrollments will display future patterns consistent with past patterns. This method implicitly includes the net effect of such factors as migration, dropouts, deaths, non-promotion, and transfers between public and private schools.\n\n\nLimitations of Projections\nProjections are complicated by the onset of the coronavirus pandemic in 2020. Projections are based on the assumption that historical patterns will continue into the future. This presents challenges both for (1) using prepandemic historical data to predict unprecedented pandemic-era behaviors and (2) using pandemic-era data to predict post-pandemic behaviors. This exercise includes both scenarios.\nEven without a pandemic, projections of a time series usually differ from the final reported data due to errors from many sources, such as the properties of the projection methodologies, which depend on the validity of many assumptions. These projections should be interpreted with caution.\n\n\n\nProcedures and Equations\nThe notation and equations that follow describe the basic procedures used to project elementary and secondary enrollments in each of the three elementary and secondary enrollment projection models.\nLet:\n\\(i\\) = Subscript denoting age\n\\(j\\) = Subscript denoting grade\n\\(t\\) = Subscript denoting time\n\\(T\\) = Subscript of the first year in the forecast period\n\\(N_t\\) = Enrollment at the prekindergarten (nursery) level\n\\(K_t\\) = Enrollment at the kindergarten level\n\\(G_{j,t}\\) = Enrollment\n\\(E_t\\) = Enrollment in elementary ungraded programs\n\\(S_t\\) = Enrollment in secondary ungraded programs\n\\(P_{i,t}\\) = Population\n\\(R_{j,t}\\) = Progression rate\n\\(RN_t\\) = Enrollment rate for prekindergarten (nursery school)\n\\(RK_t\\) = Enrollment rate for kindergarten\n\\(RG_{1,t}\\) = Enrollment rate for grade 1\n\\(RE_t\\) = Enrollment rate for elementary ungraded programs\n\\(RS_t\\) = Enrollment rate for secondary ungraded programs.\nStep 1. Calculate historical grade progression rates for each of grade. The first step in projecting the enrollments using the grade progression method was to calculate, for each grade, a progression rate for each year of actual data used to produce the projections except for the first year. The progression rate for grade \\(j\\) in year \\(t\\) equals \\[R_{j,t} = \\frac{G_{j,t}}{G_{j-1,t-1}}\\]Step 2. Produce a projected progression rate for each of grades 2 through 12. Projections for each grade’s progression rate were then produced for the forecast period using single exponential smoothing. A separate smoothing constant, chosen to minimize the sum of squared forecast errors, was used to calculate the projected progression rate for each grade. Single exponential smoothing produces a single forecast for all years in the forecast period. Therefore, for each grade \\(j\\), the projected progression rate, \\(\\hat{R}_j\\), is the same for each year in the forecast period.\nStep 3. Calculate enrollment projections for each of grades 2 through 12. For the first year in the forecast period, \\(T\\), enrollment projections, \\(\\hat{G}_{j,T}\\), for grades 2 though 12 were produced using the projected progression rates and enrollments of grades 1 though 11 from the last year of actual data, \\(T–1\\). Specifically, \\[\\hat{G}_{j,T} = \\hat{R}_j \\cdot \\hat{G}_{j-1, T-1}\\]This same procedure was then used to produce the projections for the following year, \\(T+1\\), except that enrollment projections for year \\(T\\) were used rather than actual numbers: \\[\\hat{G}_{j,T+1} = \\hat{R}_j \\cdot \\hat{G}_{j,T}\\] The enrollment projections for grades 2 through 11 for year \\(T\\) were those just produced using the grade progression method. The projection for grade 1 for year \\(T\\) was produced using the enrollment rate method as outlined in steps 4, 5, and 6 below.\nThe same procedure was used for the remaining years in the projections period.\nStep 4. Calculate historical enrollment rates for prekindergarten, kindergarten, grade 1, elementary ungraded, and secondary ungraded. The first step in projecting prekindergarten, kindergarten, first-grade, elementary ungraded, and secondary ungraded enrollments using the enrollment rate method was to calculate enrollment rates for each enrollment category for the last year of actual data, \\(T–1\\), where: \\[RN_t = \\frac{N_t}{P_{5,t}}\\] \\[RK_t = \\frac{K_t}{P_{5,t}}\\] \\[RG_{1,t} = \\frac{G_{1,t}} {P_{6,t}}\\] \\[RE_t = \\frac{E_t}{\\sum_{i=5}^{13}P_{i,t}}\\] \\[RS_t = \\frac{S_t}{\\sum_{i=14}^{17}P_{i,t}}\\] Step 5. Produce a projected enrollment rate for prekindergarten, kindergarten, grade 1, elementary ungraded, and secondary ungraded. Projections for each category’s enrollment rate were produced for the forecast period using single exponential smoothing. A separate smoothing constant, chosen to minimize the sum of squared forecast errors, was used to calculate the projected enrollment rate for each of these grades (or ungraded levels), specifically for prekindergarten, kindergarten, grade 1, elementary ungraded, and secondary ungraded. Single exponential smoothing produces a single forecast for all years in the forecast period. These enrollment rates were then used as the projected enrollment rates for each year in the forecast period (\\(\\hat{RN}\\), \\(\\hat{RK}\\), \\(\\hat{RG}_1\\), \\(\\hat{RE}\\), and \\(\\hat{RS}\\)).\nStep 6. Calculate enrollment projections for prekindergarten through grad 1 and the ungraded categories. For each year in the forecast period, the enrollment rates were then multiplied by the appropriate population projections (\\(\\hat{P_{i,t}}\\)) to calculate enrollment projections for prekindergarten (\\(\\hat{N_t}\\)), kindergarten (\\(\\hat{K_t}\\)), first grade (\\(\\hat{G}_{1,t}\\)), elementary ungraded (\\(\\hat{E}_t\\)), and secondary ungraded (\\(\\hat{S}_t\\)).\\[\\hat{N}_t = \\hat{RN} \\cdot \\hat{P}_{5,t}\\] \\[\\hat{K}_t = \\hat{RK} \\cdot \\hat{P}_{5,t}\\] \\[\\hat{G}_{1,t} = \\hat{RG}_1 \\cdot \\hat{P}_{6,t}\\] \\[\\hat{E}_t = \\hat{RE} \\cdot \\sum_{i=5}^{13}\\hat{P}_{i,t}\\] \\[\\hat{S}_t = \\hat{RS} \\cdot \\sum_{i=14}^{17}\\hat{P}_{i,t}\\] Step 7. Calculate total elementary and secondary enrollments by summing the projections for each grade and the ungraded categories. To obtain projections of total enrollment, projections of enrollments for the individual grades, elementary ungraded, and secondary ungraded were summed.\nBy following these steps and utilizing the provided methodologies, I can project national public school enrollments from 2021 through 2030 with a reasonable degree of accuracy."
  },
  {
    "objectID": "work/2024/06/26/enrollment/index.html#tools-utilized",
    "href": "work/2024/06/26/enrollment/index.html#tools-utilized",
    "title": "Projecting Elementary and Secondary School Enrollment",
    "section": "Tools Utilized",
    "text": "Tools Utilized\nThere are numerous tools to select from when tackling a project like this. Instead of delving into the various options, I decided to simply tell you what I used:\n\nMicrosoft Excel\nPosit RStudio Desktop1\n\nThis required an installation of R, the programming language.\n\n\n\n\n\n\n\n\nTipGetting Started With R\n\n\n\nIf you’re new to R, there are many resources out there to help you get started. I recommend R for the Rest of Us, particularly their Getting Started With R and Fundamentals of R courses."
  },
  {
    "objectID": "work/2024/06/26/enrollment/index.html#enrollment-projection-process",
    "href": "work/2024/06/26/enrollment/index.html#enrollment-projection-process",
    "title": "Projecting Elementary and Secondary School Enrollment",
    "section": "Enrollment Projection Process",
    "text": "Enrollment Projection Process\n\nProcess Overview\nThe process for forecasting student enrollment by grade level and total enrollment using historical data begins by viewing the enrollment data in Excel and making some minor adjustments before loading it into RStudio. It calculates enrollment rates for various grades based on historical and projected population data, then applies exponential smoothing to the historical data to create a trend. Using historical progression rates, future enrollments from 2021 to 2030 are projected. It combines actual and projected data, filters for the years 2010 to 2030, and creates aggregate columns for PK-Grade 8, Grades 9-12, and Total enrollments. The data is reshaped for plotting, and a line plot is generated. The plot visualizes enrollment trends, distinguishing between actual and projected data.\n\n\nData Used\nFor this exercise, I utilized data from the NCES’s Digest of Education Statistics’ Enrollment in public elementary and secondary schools, by level and grade: Selected years, fall 1980 through fall 2030. The dataset covers annual data from 1990 to 2020. it also includes data from 1980, 1985, and projections from 2021 through 2030 which I chose to ignore.\nI also utilized Table B-1 and Table B-2 from the NCES’s Projections of Education Statistics to 2030. Table B-1 contains data on the population of prekindergarten- and kindergarten-age children from 2010 through 2030. Table B-2 contains data on the school-age population from 2010 through 2030, with the values. Both datasets include actual values for the years 2010 through 2020 and projected values from 2021 onward (Irwin et al. 2024).\n\n\nData Loading and Preparation\nTo use the data, I initially had to modify the spreadsheet in Excel. First, I un-merged cells A3:A4, B3:B4, and D4:E4. Then, I moved “Year” and “All” from cells A3 and B3 to cells A4 and B4. I also had to modify the remaining cells in row 4:\n\nCell C4 from “Total” to “Total PK-8”\nCell D4 from “Prekinder- garten” to “Prekindergarten”\nCell F4 from ” Kinder-garten” to “Kindergarten”\nCell G4 from ” 1st grade” to “1st grade”\nCell H4 from ” 2nd grade” to “2nd grade”\nCell I4 from ” 3rd grade” to “3rd grade”\nCell J4 from ” 4th grade” to “4th grade”\nCell K4 from ” 5th grade” to “5th grade”\nCell L4 from ” 6th grade” to “6th grade”\nCell M4 from ” 7th grade” to “7th grade”\nCell N4 from ” 8th grade” to “8th grade”\nCell O4 from “Un- graded\\1” to “Ungraded PK-8”\nCell P4 from “Total” to “Total 9-12”\nCell Q4 from ” 9th grade” to “9th grade”\nCell R4 from ” 10th grade” to “10th grade”\nCell S4 from ” 11th grade” to “11th grade”\nCell T4 from ” 12th grade” to “12th grade”\nCell U4 from “Un- graded\\1,2” to “Ungraded 9-12”\n\nAdditionally, I had to delete “\\4” from cell A37. Finally, I deleted row 5 and I converted the spreadsheet from the .xls file format to .xlsx by saving it as .xlsx. After the data was in a usable state, I moved the project to RStudio.\nOnce in RStudio, the script begins by loading the readxl library for reading Excel files. The dplyr library is also loaded to facilitate data manipulation. The enrollment data is then read from an Excel file, specifically from a range of cells (A4:U37), with certain columns skipped, and the remaining columns are renamed for convenience. Similarly, population data, which includes projected school-age population by selected age groups from 2010 through 2030, is read from another Excel file. To calculate enrollment rates, the enrollment data is joined with the population data on the Year column. This allows the script to compute the enrollment rates for Prekindergarten, Kindergarten, 1st grade, and ungraded enrollments based on the respective age groups in the population data. The mean enrollment rates are then computed, which will be used for forecasting future enrollments.\n\n\n\n\n\n\nNoteClick here to view the code I used.\n\n\n\n\n\n\n# Load necessary libraries\nlibrary(readxl)\nlibrary(dplyr)\n\n# Load enrollment data\nenrollment &lt;- read_excel(\n  \"data/tabn203.10.xlsx\",\n  sheet = \"Digest 2021 Table 203.10\",\n  range = \"A4:U37\",\n  col_types = c(\n    \"numeric\",\n    \"skip\",\n    \"skip\",\n    \"numeric\",\n    \"skip\",\n    \"numeric\",\n    \"numeric\",\n    \"numeric\",\n    \"numeric\",\n    \"numeric\",\n    \"numeric\",\n    \"numeric\",\n    \"numeric\",\n    \"numeric\",\n    \"numeric\",\n    \"skip\",\n    \"numeric\",\n    \"numeric\",\n    \"numeric\",\n    \"numeric\",\n    \"numeric\"\n  )\n)\n\n# Rename the columns for convenience\ncolnames(enrollment) &lt;- c(\n  \"Year\",\n  \"Prekindergarten\",\n  \"Kindergarten\",\n  \"1st_grade\",\n  \"2nd_grade\",\n  \"3rd_grade\",\n  \"4th_grade\",\n  \"5th_grade\",\n  \"6th_grade\",\n  \"7th_grade\",\n  \"8th_grade\",\n  \"9th_grade\",\n  \"10th_grade\",\n  \"11th_grade\",\n  \"12th_grade\",\n  \"Ungraded_PK_8\",\n  \"Ungraded_9_12\"\n)\n\n# Load population data\npopulation &lt;- read_excel(\n  \"data/actual_and_projected_school-age_populations_by_selected_ages-_2010_through_2030.xlsx\",\n  sheet = \"Sheet1\",\n  col_types = c(\n    \"numeric\",\n    \"numeric\",\n    \"numeric\",\n    \"numeric\",\n    \"numeric\",\n    \"numeric\",\n    \"numeric\",\n    \"numeric\"\n  )\n)\n\n# Calculate enrollment rates for Prekindergarten, Kindergarten, 1st grade, and Ungraded enrollments\nenrollment_rates &lt;- enrollment %&gt;%\n  inner_join(population, by = \"Year\") %&gt;%\n  mutate(\n    Prekindergarten_rate = Prekindergarten / `3- to 5-year-olds`,\n    Kindergarten_rate = Kindergarten / `5-year-olds`,\n    first_grade_rate = `1st_grade` / `6-year-olds`,\n    Ungraded_PK_8_rate = Ungraded_PK_8 / `5- to 13-year-olds`,\n    Ungraded_9_12_rate = Ungraded_9_12 / `14- to 17-year-olds`\n  ) %&gt;%\n  summarise(\n    Prekindergarten_rate = mean(Prekindergarten_rate, na.rm = TRUE),\n    Kindergarten_rate = mean(Kindergarten_rate, na.rm = TRUE),\n    first_grade_rate = mean(first_grade_rate, na.rm = TRUE),\n    Ungraded_PK_8_rate = mean(Ungraded_PK_8_rate, na.rm = TRUE),\n    Ungraded_9_12_rate = mean(Ungraded_9_12_rate, na.rm = TRUE)\n  )\n\n\n\n\n\n\nForecasting Functions\nIn this section, the script defines an exponential smoothing function to create smoothed trends from historical enrollment data. This function, exponential_smoothing, takes a time series and a smoothing parameter, alpha, as inputs. It initializes the result with the first value of the series and then iteratively applies the exponential smoothing formula: each subsequent value is a weighted average of the current actual value and the previous smoothed value. This technique helps reduce variability in the data, making trends more apparent and stable for forecasting purposes. The smoothed values produced by this function are crucial for generating accurate and reliable projections of future enrollments based on historical trends.\n\n\n\n\n\n\nNoteClick here to view the code I used.\n\n\n\n\n\n\n# Define exponential smoothing function to return full series\nexponential_smoothing &lt;- function(series, alpha = 0.3) {\n  result &lt;- numeric(length(series))\n  result[1] &lt;- series[1]  # Initialize with the first value\n  for (i in 2:length(series)) {\n    result[i] &lt;- alpha * series[i] + (1 - alpha) * result[i - 1]\n  }\n  return(result)\n}\n\n\n\n\n\n\nForecasting Enrollment\nHere, the script utilizes the previously defined exponential smoothing function and calculated enrollment rates to project future enrollments from 2021 to 2030. Initially, the historical enrollment data is smoothed to reduce variability and create stable trends. A new dataframe is then created to hold projected enrollment values for each grade level. The script calculates progression rates, which represent the average proportion of students progressing from one grade to the next based on historical data. Using these rates, the script populates the projections dataframe, starting with the last smoothed value from the historical data and applying the progression rates for subsequent years. Specific projections for Prekindergarten, Kindergarten, 1st grade, and ungraded enrollments are computed using the enrollment rates and projected population data. This detailed approach ensures that the projections are grounded in historical trends and population forecasts, providing a comprehensive view of future enrollment patterns.\n\n\n\n\n\n\nNoteClick here to view the code I used.\n\n\n\n\n\n\n# Apply exponential smoothing to historical data\nsmoothed_values &lt;- enrollment %&gt;%\n  select(-Year) %&gt;%\n  mutate(across(everything(), ~ exponential_smoothing(.x, alpha = 0.3)))\n\n# Initialize projections dataframe\nyears &lt;- 2021:2030\nprojections &lt;- data.frame(Year = years)\n\n# Calculate progression rates for grades 2 through 12\nprogression_rates &lt;- enrollment %&gt;%\n  summarise(across(starts_with(\"grade\"), list(rate = ~ mean(\n    .x / lag(.x, 1), na.rm = TRUE\n  )), .names = \"{col}_rate\"))\n\n# Populate the projections dataframe for grades with historical progression rates\nfor (grade in names(smoothed_values)) {\n  projections[[grade]] &lt;- NA\n  projections[1, grade] &lt;- tail(smoothed_values[[grade]], 1) # Initial value from smoothed historical data\n  \n  for (year_index in 2:length(years)) {\n    rate &lt;- progression_rates[[paste0(grade, \"_rate\")]]\n    if (!is.null(rate) && !is.na(rate)) {\n      projections[year_index, grade] &lt;- projections[year_index - 1, grade] * rate\n    } else {\n      projections[year_index, grade] &lt;- projections[year_index - 1, grade]\n    }\n  }\n}\n\n# Project Prekindergarten, Kindergarten, 1st grade, and Ungraded enrollments using enrollment rates\nfor (year_index in 1:length(years)) {\n  year &lt;- years[year_index]\n  projections[year_index, \"Prekindergarten\"] &lt;- population[population$Year == year, \"3- to 5-year-olds\"] * enrollment_rates$Prekindergarten_rate\n  projections[year_index, \"Kindergarten\"] &lt;- population[population$Year == year, \"5-year-olds\"] * enrollment_rates$Kindergarten_rate\n  projections[year_index, \"1st_grade\"] &lt;- population[population$Year == year, \"6-year-olds\"] * enrollment_rates$first_grade_rate\n  projections[year_index, \"Ungraded_PK_8\"] &lt;- population[population$Year == year, \"5- to 13-year-olds\"] * enrollment_rates$Ungraded_PK_8_rate\n  projections[year_index, \"Ungraded_9_12\"] &lt;- population[population$Year == year, \"14- to 17-year-olds\"] * enrollment_rates$Ungraded_9_12_rate\n}\n\n\n\n\n\n\nForecast Table\nIn this section, the gt library is loaded to facilitate the creation of well-formatted tables. The code takes the processed enrollment data and organizes it into a clear, easy-to-read format. It shows enrollment numbers for total students, prekindergarten through grade 8, and grades 9 through 12, with all numbers formatted as whole integers. The data is implicitly divided into “Actual” and “Projected” sections. The code applies various styling elements to enhance readability, such as right-aligning numeric columns, setting specific column widths, and using light gray backgrounds for row groups and column labels. The resulting table presents the enrollment data in a professional, organized manner that’s easy for viewers to understand and analyze.\n\n\n\n\n\n\nNoteClick here to view the code I used.\n\n\n\n\n\n\n# Load `gt` library\nlibrary(gt)\n\n# Process the data\nprocessed_data &lt;- plot_data %&gt;%\n  pivot_wider(names_from = Category, values_from = Enrollment) %&gt;%\n  mutate(Type = ifelse(Year &lt;= 2020, \"Actual\", \"Projected\"), across(c(Total, PK_Grade_8, Grades_9_12), ~ . * 1000)) %&gt;%\n  select(Year, Type, Total, PK_Grade_8, Grades_9_12)\n\n# Create the table\nenrollment_table &lt;- processed_data %&gt;%\n  gt(groupname_col = \"Type\") %&gt;%\n  tab_header(title = \"School Enrollment Projections\", subtitle = \"2010-2030\") %&gt;%\n  fmt_number(columns = c(Total, PK_Grade_8, Grades_9_12),\n             decimals = 0) %&gt;%\n  cols_label(PK_Grade_8 = \"Pre-K through grade 8\", Grades_9_12 = \"Grades 9 through 12\") %&gt;%\n  cols_align(align = \"right\",\n             columns = c(Total, PK_Grade_8, Grades_9_12)) %&gt;%\n  cols_width(Year ~ px(80),\n             Total ~ px(120),\n             PK_Grade_8 ~ px(200),\n             Grades_9_12 ~ px(160)) %&gt;%\n  tab_options(\n    row_group.background.color = \"#F0F0F0\",\n    column_labels.background.color = \"#E0E0E0\",\n    table.border.top.style = \"hidden\",\n    table.border.bottom.style = \"hidden\",\n    data_row.padding = px(5)\n  ) %&gt;%\n  cols_hide(columns = c(Type))  # Hide the Type column\n\n# Display the table\nenrollment_table\n\n\n\n\n\n\n\n\n\n\n\n\nPublic School Enrollment Over Time and During the Pandemic\n\n\n2010-2030\n\n\nYear\nTotal\nPre-K through grade 8\nGrades 9 through 12\n\n\n\n\nActual\n\n\n2010\n49,484,181\n38,019,738\n11,464,443\n\n\n2011\n49,521,669\n38,148,111\n11,373,558\n\n\n2012\n49,771,118\n38,419,219\n11,351,899\n\n\n2013\n50,044,522\n38,641,582\n11,402,940\n\n\n2014\n50,312,581\n38,778,978\n11,533,603\n\n\n2015\n50,438,043\n38,838,581\n11,599,462\n\n\n2016\n50,615,189\n38,960,887\n11,654,302\n\n\n2017\n50,685,567\n39,038,737\n11,646,830\n\n\n2018\n50,694,061\n39,054,837\n11,639,224\n\n\n2019\n50,796,445\n39,079,696\n11,716,749\n\n\n2020\n49,375,467\n37,632,044\n11,743,423\n\n\nProjected\n\n\n2021\n50,238,964\n38,569,795\n11,669,168\n\n\n2022\n50,084,028\n38,414,783\n11,669,245\n\n\n2023\n49,925,057\n38,255,996\n11,669,061\n\n\n2024\n49,749,962\n38,081,082\n11,668,880\n\n\n2025\n49,612,468\n37,943,840\n11,668,627\n\n\n2026\n49,244,974\n37,576,673\n11,668,301\n\n\n2027\n48,919,455\n37,251,149\n11,668,306\n\n\n2028\n49,062,110\n37,393,774\n11,668,336\n\n\n2029\n49,258,554\n37,590,160\n11,668,394\n\n\n2030\n49,298,918\n37,630,442\n11,668,476\n\n\n\n\n\n\n\n\n\nData Combination and Visualization\nThis R code performs a series of data manipulation and visualization tasks to create a line plot that displays the projected enrollment. It starts by combining actual enrollment data with projected figures, focusing on the years 2010 to 2030. The data is then organized into three main categories: PK-Grade 8, Grades 9-12, and Total enrollment. Using the ggplot2 library, the code generates a line plot that displays these enrollment numbers over time, measured in millions of students. The visualization distinguishes between historical data and future projections by using solid lines for actual data up to 2021 and dashed lines for projected data beyond that point. A vertical line at 2021 further emphasizes this distinction, with annotations clarifying which sections represent actual versus projected data. The plot uses different colors for each enrollment category, making it easy to differentiate between total enrollment and the two grade range subgroups. The x-axis spans from 2010 to 2030, while the y-axis shows enrollment figures. Various styling elements are applied to enhance the plot’s readability and visual appeal, including a centered title, a legend at the bottom, and angled x-axis labels to prevent overlapping. This comprehensive visualization allows viewers to easily compare enrollment trends across different grade ranges and see how these trends are expected to evolve in the coming years, providing a valuable tool for educational planning and analysis.\n\n\n\n\n\n\nNoteClick here to view the code I used.\n\n\n\n\n\n\n# Load `tidyr` and `ggplot2` libraries\nlibrary(tidyr)\nlibrary(ggplot2)\n\n# Combine actual and projected data\ncombined_data &lt;- bind_rows(enrollment %&gt;% mutate(Type = \"Actual\"),\n                           projections %&gt;% mutate(Type = \"Projected\"))\n\n# Filter data for the years 2010 to 2030\nfiltered_data &lt;- combined_data %&gt;% filter(Year &gt;= 2010 &\n                                            Year &lt;= 2030)\n\n# Arrange the data in the required format and sort by Year\nfiltered_data &lt;- filtered_data %&gt;% arrange(Year)\n\n# Create new columns for PK-Grade 8, Grades 9-12, and Total\nfiltered_data &lt;- filtered_data %&gt;%\n  mutate(\n    PK_Grade_8 = Prekindergarten + Kindergarten + `1st_grade` + `2nd_grade` + `3rd_grade` + `4th_grade` + `5th_grade` + `6th_grade` + `7th_grade` + `8th_grade` + Ungraded_PK_8,\n    Grades_9_12 = `9th_grade` + `10th_grade` + `11th_grade` + `12th_grade` + Ungraded_9_12,\n    Total = PK_Grade_8 + Grades_9_12\n  )\n\n# Create a long format dataframe for plotting\nplot_data &lt;- filtered_data %&gt;%\n  select(Year, PK_Grade_8, Grades_9_12, Total) %&gt;%\n  pivot_longer(\n    cols = c(PK_Grade_8, Grades_9_12, Total),\n    names_to = \"Category\",\n    values_to = \"Enrollment\"\n  ) %&gt;%\n  mutate(Category = factor(Category, levels = c(\"Total\", \"PK_Grade_8\", \"Grades_9_12\")))\n\n# Create the line plot\nenrollment_plot &lt;- ggplot(plot_data, aes(x = Year, y = Enrollment / 1000, color = Category)) +\n  # Solid lines up to 2021\n  geom_line(data = subset(plot_data, Year &lt;= 2021), linewidth = 1.2) +\n  # Dashed lines from 2021 onward\n  geom_line(data = subset(plot_data, Year &gt;= 2021), linewidth = 1.2, linetype = \"dashed\") +\n  geom_vline(xintercept = 2021, color = \"gray50\") +\n  annotate(\n    \"text\",\n    x = 2015,\n    y = max(plot_data$Enrollment) / 1000 * 0.9,\n    label = \"Actual\",\n    size = 4\n  ) +\n  annotate(\n    \"text\",\n    x = 2025,\n    y = max(plot_data$Enrollment) / 1000 * 0.9,\n    label = \"Projected\",\n    size = 4\n  ) +\n  scale_color_manual(\n    values = c(\n      \"Total\" = \"#009E73\",\n      \"PK_Grade_8\" = \"#E69F00\",\n      \"Grades_9_12\" = \"#56B4E9\"\n    ),\n    labels = c(\"Total\", \"PK-Grade 8\", \"Grades 9-12\")\n  ) +\n  scale_x_continuous(breaks = 2010:2030, expand = c(0, 0)) +\n  scale_y_continuous(\n    breaks = seq(0, 50, by = 10),\n    limits = c(5, 55),\n    expand = c(0, 0)\n  ) +\n  coord_cartesian(xlim = c(2010, 2030)) +\n  labs(title = \"Enrollment in public elementary and secondary schools, by level\", x = \"Year\", y = \"Enrollment (in millions)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    legend.title = element_blank(),\n    legend.position = \"bottom\",\n    axis.text.x = element_text(\n      angle = 45,\n      hjust = 1,\n      size = 8\n    ),\n    panel.grid.minor = element_blank()\n  )\n\n# Print the plot\nprint(enrollment_plot)"
  },
  {
    "objectID": "work/2024/06/26/enrollment/index.html#conclusion",
    "href": "work/2024/06/26/enrollment/index.html#conclusion",
    "title": "Projecting Elementary and Secondary School Enrollment",
    "section": "Conclusion",
    "text": "Conclusion\nThis comprehensive methodology for projecting student enrollment in elementary and secondary schools provides a robust framework for educational planners and policymakers. By utilizing exponential smoothing techniques, grade progression rates, and enrollment rate methods, this approach offers a nuanced view of future enrollment trends from 2021 through 2030.\nThe process outlined here, from data preparation to visualization, allows for a detailed analysis of enrollment patterns across different grade levels and age groups. It takes into account historical trends, demographic shifts, and the potential long-term impacts of unprecedented events like the COVID-19 pandemic.\nWhile these projections offer valuable insights for resource allocation, budgeting, and long-term planning in education, it’s crucial to interpret them with caution. The inherent limitations of forecasting, particularly in the face of unforeseen circumstances, underscore the need for regular updates and adaptations to these projections.\nBy combining rigorous statistical methods with clear data visualization, this approach not only aids in decision-making processes but also facilitates effective communication of enrollment trends to stakeholders. As the education landscape continues to evolve, such data-driven projections will play an increasingly vital role in shaping responsive and resilient educational systems.\n\nReferences\n\n\nIrwin et al. 2024. “Projections of Education Statistics to 2030.” NCES 2024-034. Washington, DC: U.S. Department of Education; National Center for Education Statistics."
  },
  {
    "objectID": "work/2024/06/26/enrollment/index.html#footnotes",
    "href": "work/2024/06/26/enrollment/index.html#footnotes",
    "title": "Projecting Elementary and Secondary School Enrollment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRStudio Desktop is a standalone opensource application that does not store information on the internet. RStudio Desktop Pro, on the other hand, can or does. I’m not sure. If this is a concern for you, speak with Posit. They were very helpful helping the Florida UFSD determine whether their software is EdLaw 2-d compliant, which is always a concern when vetting new applications.↩︎"
  },
  {
    "objectID": "work/2025/10/01/openaiapi/index.html",
    "href": "work/2025/10/01/openaiapi/index.html",
    "title": "Calling the OpenAI API from an IDE",
    "section": "",
    "text": "You can use Python scripting to call the OpenAI API and get a short, focused response from a chat model. While there are cloud-based environments like Google Colab, which is a Jupyter Notebook environment, I prefer to use a traditional software application installed directly on my computer called an integrated development environment (IDE). There are several reasons I like to work with an IDE, including having complete control over my environment, seamless integration of local files, and the ability to work offline. There are many IDEs to choose from, such as Visual Studio Code (often referred to as VS Code) and PyCharm, which offer built-in support for Jupyter Notebooks. I like to use Positron, which takes the modern, extensible foundation of VS Code and combines it with data-centric features. If you’re interested in learning how I called the OpenAI API from Positron, please read on."
  },
  {
    "objectID": "work/2025/10/01/openaiapi/index.html#introduction",
    "href": "work/2025/10/01/openaiapi/index.html#introduction",
    "title": "Calling the OpenAI API from an IDE",
    "section": "",
    "text": "You can use Python scripting to call the OpenAI API and get a short, focused response from a chat model. While there are cloud-based environments like Google Colab, which is a Jupyter Notebook environment, I prefer to use a traditional software application installed directly on my computer called an integrated development environment (IDE). There are several reasons I like to work with an IDE, including having complete control over my environment, seamless integration of local files, and the ability to work offline. There are many IDEs to choose from, such as Visual Studio Code (often referred to as VS Code) and PyCharm, which offer built-in support for Jupyter Notebooks. I like to use Positron, which takes the modern, extensible foundation of VS Code and combines it with data-centric features. If you’re interested in learning how I called the OpenAI API from Positron, please read on."
  },
  {
    "objectID": "work/2025/10/01/openaiapi/index.html#what-youll-learn",
    "href": "work/2025/10/01/openaiapi/index.html#what-youll-learn",
    "title": "Calling the OpenAI API from an IDE",
    "section": "What You’ll Learn",
    "text": "What You’ll Learn\nBy following this guide, you’ll master several crucial skills for working with APIs in educational contexts. You’ll learn how to securely store and access API keys using environment variables, which is a fundamental practice in professional software development. You’ll learn to structure your code using functions, making your programs more reusable and maintainable. Most importantly, you’ll discover how to implement proper error handling so your programs respond gracefully when things go wrong, rather than crashing unexpectedly."
  },
  {
    "objectID": "work/2025/10/01/openaiapi/index.html#understanding-api-security-why-secrets-matter",
    "href": "work/2025/10/01/openaiapi/index.html#understanding-api-security-why-secrets-matter",
    "title": "Calling the OpenAI API from an IDE",
    "section": "Understanding API Security: Why Secrets Matter",
    "text": "Understanding API Security: Why Secrets Matter\nBefore we start, it’s crucial to understand the process of storing your secret API key in a separate file, having your code securely load it without exposing the key in the script itself, and why this process is necessary. Think of your API key like a credit card number that allows you to make purchases from OpenAI’s services. Hard-coding your API key directly into your script is akin to writing your credit card number on a sticky note and leaving it on your desk, where anyone can see it.\nIf you accidentally share your code or upload it to a public repository like GitHub, your secret key will be exposed, allowing anyone to use your account and incur charges on your behalf. We need a strategy to keep our API keys secure.\nHere is where environment variables come into play. Environment variables are like labeled containers that store information your program can access without that information being visible in your code itself. Think of them as a secure filing cabinet that only your program can open. The information lives separately from your code, so you can safely share your scripts without exposing your secrets.\nFortunately, there are several ways we can manage this securely. We’ll use a tool called dotenv, which helps you manage application secrets and configuration by loading them from a special file into your program’s environment variables."
  },
  {
    "objectID": "work/2025/10/01/openaiapi/index.html#calling-the-openai-api-from-an-ide",
    "href": "work/2025/10/01/openaiapi/index.html#calling-the-openai-api-from-an-ide",
    "title": "Calling the OpenAI API from an IDE",
    "section": "Calling the OpenAI API from an IDE",
    "text": "Calling the OpenAI API from an IDE\n\nStep 1: Get Your API Key 🔑\nFirst, you need to get your API key from the OpenAI platform.\n\nGo to platform.openai.com and log in.\nNavigate to Dashboard &gt; API keys.\nClick “Create new secret key” and create one if you haven’t already.\nCopy the key immediately and save it somewhere safe.\n\nI used 1Password for this, but any secure password manager will work.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou won’t be able to see your secret key again after you close the window.\n\n\n\n\nStep 2: Set Up Your Project Folder 📂\nCreate a new project folder. Inside that folder, create two files:\n\nmain.py: This will be your Python script.\n\nYou can name this file anything you want, such as chatbot.py or api_test.py.\n\n.env: This is where you’ll store your secret key. The . at the beginning makes it a hidden file on many systems, which provides an additional layer of protection.\n\nYour folder should look like this:\n\nmy_project/\n├── main.py\n└── .env\n\n\n\nStep 3: Install the Necessary Libraries\nYou’ll need two Python libraries: openai to interact with the API and python-dotenv to load your .env file. Think of these as specialized tools that handle the complex work of communicating with OpenAI’s servers and managing your environment variables, respectively.\nOpen your terminal in the IDE and run:\n\npip install openai python-dotenv\n\nIf you’re new to using pip, this command tells Python’s package manager to download and install these libraries so your code can use them. The openai library contains pre-written functions that handle all the technical details of making API requests, while python-dotenv provides a simple way to load configuration from files.\n\n\nStep 4: Add Your API Key to the .env File\nOpen the .env file you created and add your API key in the following format. Notice there are no quotes around the value, just the variable name, an equals sign, and the key itself:\n\nOPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n\nReplace sk-xxxx… with the actual secret key you copied from OpenAI. You can name the OPENAI_API_KEY variable anything you want, but using a descriptive name helps you remember what it contains. Some developers may prefer a name like EDUC6192_OPENAI_API_KEY to indicate which project or course the key belongs to.\nThe .env file acts like a secure configuration file that stays on your computer but never gets shared with your code. When you run your program, the dotenv library will read this file and make these variables available to your Python script.\n\n\nStep 5: Write the Python Code 💻\nOpen your main.py file and write the Python code. It will first load the variables from the .env file and then use the key to make an API call.\n\n\n\n\n\n\nNoteClick here to view the code I wrote with detailed explanations.\n\n\n\n\n\n\n# Import necessary libraries\nimport os  # For accessing environment variables\nfrom dotenv import load_dotenv  # For loading .env file\nfrom openai import OpenAI  # For making API calls\n\n# Load environment variables from .env file\n# This must be called before trying to access any variables\nload_dotenv()\n\ndef get_chatbot_response(user_prompt, model=\"gpt-4o-mini\"):\n    \"\"\"\n    Function to get a response from OpenAI's chat models.\n    \n    This function encapsulates the API logic in a reusable way\n    that can be called multiple times with different prompts.\n    \n    Args:\n        user_prompt (str): The question or prompt to send to the AI\n        model (str): The specific AI model to use (defaults to gpt-4o-mini)\n    \n    Returns:\n        str: The AI's response or an error message\n    \"\"\"\n    \n    # Load your API key from an environment variable\n    api_key = os.getenv(\"OPENAI_API_KEY\")\n    \n    # Error handling: Check if the API key was successfully loaded\n    # This prevents crashes and gives helpful feedback to the user\n    if not api_key:\n        return \"Error: OPENAI_API_KEY environment variable not set. Check your .env file.\"\n\n    try:\n        # Initialize the OpenAI client with our API key\n        # This creates a connection object that handles all the technical details\n        client = OpenAI(api_key=api_key)\n\n        # Create a chat completion request\n        # This is where we actually send our prompt to OpenAI's servers\n        response = client.chat.completions.create(\n            model=model,  # Specify which AI model to use\n            messages=[\n                {\n                    # System message: Sets the AI's role and behavior\n                    # This is like giving the AI a job description\n                    \"role\": \"system\",\n                    \"content\": (\n                        \"You are a teaching assistant for an LLM Applications in Education course. \"\n                        \"You provide insightful answers about implementing large language models \"\n                        \"in educational contexts and explain technical concepts clearly with \"\n                        \"practical, real-world examples.\"\n                    ),\n                },\n                {\n                    # User message: The actual question or prompt\n                    \"role\": \"user\",\n                    \"content\": user_prompt,\n                },\n            ],\n            max_tokens=50,  # Limit response length (1 token ≈ 0.75 words)\n            temperature=0.2  # Low temperature = more focused, consistent responses\n                           # Higher values (up to 1.0) = more creative, varied responses\n        )\n        \n        # Extract and return the AI's response\n        # The response object contains metadata, but we just want the text\n        return response.choices[0].message.content\n\n    except Exception as e:\n        # Comprehensive error handling: Catch any other problems\n        # This could be network issues, API errors, or authentication problems\n        return f\"An error occurred: {e}\"\n\n# Main execution block\n# This Python convention allows the script to run directly or be imported as a module\nif __name__ == \"__main__\":\n    # Define your prompt\n    # You can easily change this or make it interactive by using input()\n    # to ask the user for questions\n    prompt = (\n        \"What are the most effective prompt engineering techniques for creating \"\n        \"educational chatbots that can adapt to different student learning levels \"\n        \"in the same classroom?\"\n    )\n    \n    # Get and print the assistant's reply\n    assistant_reply = get_chatbot_response(prompt)\n    print(f\"AI Response: {assistant_reply}\")\n\n\n\n\n\nUnderstanding the Code Structure\n\n\n\n\n\n\nNoteClick here to learn about the design principles and rationale behind my code.\n\n\n\n\n\nMy implementation uses a modular approach with a reusable function, get_chatbot_response(user_prompt, model). This function encapsulates all the logic for making the API call, which means you can easily call it multiple times with different prompts without rewriting code.\nThe code includes robust error handling that first checks if the API key was found and returns a helpful message if not. It also wraps the API call in a try...except block to gracefully catch any potential issues, like network problems or API errors, and report them without crashing.\nThe use of if __name__ == \"__main__\": is a standard Python convention that allows the script to be run directly or imported as a module into a larger application.\nI’ve chosen gpt-4o-mini as the default model for several important reasons. It’s cost-effective, which is crucial for educational projects where budget constraints are a concern. You want to maximize experimentation within limited resources. gpt-4o-mini offers excellent performance across reasoning, instruction-following, and code-generation tasks while being efficient. By using this modern model, you’re leveraging current technology, which demonstrates good practice in educational technology."
  },
  {
    "objectID": "work/2025/10/01/openaiapi/index.html#understanding-the-code-structure",
    "href": "work/2025/10/01/openaiapi/index.html#understanding-the-code-structure",
    "title": "Calling the OpenAI API from an IDE",
    "section": "Understanding the Code Structure",
    "text": "Understanding the Code Structure\n\n\n\n\n\n\nNoteClick here to learn about the design principles and rationale behind my code.\n\n\n\n\n\nMy implementation uses a modular approach with a reusable function, get_chatbot_response(user_prompt, model). This function encapsulates all the logic for making the API call, which means you can easily call it multiple times with different prompts without rewriting code.\nThe code includes robust error handling that first checks if the API key was found and returns a helpful message if not. It also wraps the API call in a try...except block to gracefully catch any potential issues, like network problems or API errors, and report them without crashing.\nThe use of if __name__ == \"__main__\": is a standard Python convention that allows the script to be run directly or imported as a module into a larger application.\nI’ve chosen gpt-4o-mini as the default model for several important reasons. It’s cost-effective, which is crucial for educational projects where budget constraints are a concern. You want to maximize experimentation within limited resources. gpt-4o-mini offers excellent performance across reasoning, instruction-following, and code-generation tasks while being efficient. By using this modern model, you’re leveraging current technology, which demonstrates good practice in educational technology."
  },
  {
    "objectID": "work/2025/10/01/openaiapi/index.html#troubleshooting-common-issues",
    "href": "work/2025/10/01/openaiapi/index.html#troubleshooting-common-issues",
    "title": "Calling the OpenAI API from an IDE",
    "section": "Troubleshooting Common Issues",
    "text": "Troubleshooting Common Issues\nWhen working with APIs, you’ll likely encounter some common problems. Here’s how to identify and solve them:\n“Error: OPENAI_API_KEY environment variable not set”: This means your .env file isn’t being read properly by the Python code. Check that the file is named exactly .env (with the dot at the beginning), is in the same folder as your Python script, and contains the line OPENAI_API_KEY=your_actual_key_here with no extra spaces.\n“An error occurred: Incorrect API key provided”: Your API key is loaded into the Python code, but it isn’t valid. Double-check that you copied the entire key correctly from the OpenAI dashboard, including the sk- prefix.\n“An error occurred: Connection error”: This usually indicates a network problem. Check your internet connection, and if you’re on a school or corporate network, you might need to configure proxy settings.\nImport errors for openai or dotenv: Make sure you’ve installed the required libraries using pip install openai python-dotenv."
  },
  {
    "objectID": "work/2025/10/01/openaiapi/index.html#next-steps",
    "href": "work/2025/10/01/openaiapi/index.html#next-steps",
    "title": "Calling the OpenAI API from an IDE",
    "section": "Next Steps",
    "text": "Next Steps\nThis foundation prepares you for more advanced API work.\nUnderstanding environment variables and error handling will serve you well, as these are fundamental practices in professional software development. The modular approach you’ve learned here scales to much larger applications and team projects.\nThe principles of secure key management and robust error handling apply to virtually any API you’ll work with in your career."
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About",
    "section": "",
    "text": "I’m a Hudson Valley resident, but I was born and raised in Philadelphia. I live with my wife, son, two dogs, and two cats. I enjoy spending time with my friends and family, experiencing music and food, and playing board and video games (❤️ Nintendo).\nI’m a data scientist specializing in K–12 education. I love working in this field because I get to use data and technology to help students succeed while making life easier for the teachers and administrators who support them every day. My biggest passion is building systems that improve student outcomes fairly—without the algorithmic bias that can creep into educational technology.\nI graduated from the State University of New York at New Paltz in 2024 with a Bachelor of Science in Business Analytics. I’m currently pursuing a Master of Science in Education in Learning Analytics and Artificial Intelligence at the University of Pennsylvania Graduate School of Education. The program focuses on utilizing machine learning and AI to develop smarter, more adaptive learning technologies that can identify struggling students early and provide them with support before they fall behind. For my capstone project, I’m developing an early warning system that does precisely that, with a strong emphasis on fairness metrics to ensure it works equitably for all students.\nIf any of this sounds cool or interesting to you, I’d love to hear from you!\n\n\n Back to top"
  },
  {
    "objectID": "privacy/index.html",
    "href": "privacy/index.html",
    "title": "Privacy Policy",
    "section": "",
    "text": "Policy\nI want to ensure the privacy of my website visitors. This Privacy Policy document contains details about the types of information collected and recorded and how I utilize it. If you have any questions or require further information regarding my Privacy Policy, please do not hesitate to contact me. This Privacy Policy is valid for visitors to my website regarding the information shared and collected. Please note that this policy does not apply to any information collected offline or through channels other than this website.\n\n\nConsent\nBy using my website, you consent to my Privacy Policy and agree to its terms. Your support and understanding of my commitment to safeguarding your privacy is greatly appreciated.\n\n\nCCPA rights\nUnder the CCPA, California consumers are entitled to certain things. If you contact me regarding this, I have one month to respond. Please get in touch with me if you would like to exercise any of the following rights. - The right to request that I disclose the categories and specific personal data collected. - The right to request the deletion of any personal data about the consumer I collected. - The right to request that I do not sell their data. - Finding and preventing fraud: the information collected assists in detecting and mitigating fraudulent activities, ensuring the security and integrity of the website and its users. - Providing, operating, and maintaining my website: the collected information is essential for the overall provision, operation, and maintenance of the website, enabling its functionality and ensuring a smooth user experience.\n\n\nChildren’s information\nAnother one of my priorities is to provide protection for children while they use the internet. I strongly encourage parents and guardians to observe, participate in, monitor, and guide their children’s online activities. I do not knowingly collect any personally identifiable information from children under 13. If you believe that your child has provided such information on my website, I urge you to contact me immediately. I will make every effort to promptly remove such information from my records and ensure the privacy and safety of your child.\n\n\nCollected information\nI utilize Google Analytics to monitor the performance of my website. All site measurements are carried out in absolute anonymity when using Google Analytics. No cookies are set, and no personal data is collected. All data is aggregated for statistical purposes only. I utilize the information I collect in various ways: - Improving, personalizing, and expanding my website: the information helps me enhance your experience, tailor content to your preferences, and develop and grow my website. - Understanding and analyzing how you use my website: by analyzing certain behaviors and patterns, I gain insights that allow me to optimize the website’s functionality and performance, ensuring it meets your needs. - Finding and preventing fraud: the information collected assists in detecting and mitigating fraudulent activities, ensuring the security and integrity of the website and its users. - Providing, operating, and maintaining my website: the collected information is essential for the overall provision, operation, and maintenance of the website, enabling its functionality and ensuring a smooth user experience.\n\n\nGDPR rights\nI want to ensure you fully know your data protection rights. As a user, you are entitled to certain things, as stated below. If you make a request, I am obliged to respond within one month. Please get in touch with me if you want to exercise any of these rights. - The right to access: You can request copies of your data. - The right to rectification: You can request corrections to any information you believe is inaccurate. You also have the right to request the completion of any information you believe is incomplete. - The right to erasure: You have the right to request the erasure of your data under certain conditions. - The right to restrict processing: You can request the restriction of processing your data under certain conditions. - The right to object to processing: You have the right to object to the processing of your data under certain conditions. - The right to data portability: You can request the transfer of the data I have collected to another organization or directly to you under certain conditions.\nI adapted this page from Tamara Sredojevic.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "contact/index.html",
    "href": "contact/index.html",
    "title": "Contact",
    "section": "",
    "text": "Feel free to reach out! I’d love to hear from you.\n\n\n\n\nFull Name \n\n\nEmail Address \n\n\nMessage\n\n\n\nSend Message\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nHi, I’m John Baker. 👋\n",
    "section": "",
    "text": "Hi, I’m John Baker. 👋\n\n\nI work with teachers and school administrators to make sense of their data—from enrollment projections to student outcomes. My job is turning numbers into insights that help educators focus on what matters most: their students.\n\n\nLearn More\n\n\n\n\n Back to top"
  },
  {
    "objectID": "work/2024/04/26/garden/index.html",
    "href": "work/2024/04/26/garden/index.html",
    "title": "Welcome to My Digital Garden",
    "section": "",
    "text": "I first heard about “digital gardens” during Vicki Boykis’s rstudio::global 2021 keynote entitled “Your Public Garden.” During it, she discussed how it’s important to build a digital garden as a way to take control of your online experience and create a more positive space. This approach intrigued me, so I wanted to learn more about it. Maggie Appleton did an excellent job exploring how digital gardens serve as personal knowledge management systems, allowing individuals to cultivate their thoughts and ideas in a non-linear format. There are many benefits to planting and tending to a digital garden, such as fostering creativity, enabling serendipitous discovery, and facilitating knowledge sharing. I found this concept delightful, so I planted my own digital garden.\nBefore this, I had a website I built with Gatsby. I liked that website. The design was a fork of Simplefolio by Jacobo Martínez. I modified the theme’s colors to be more purplish-blue than sea green, changed the font, and added links to my social networks and data analysis projects. It took a while to finish the site, and I wish I documented the process. It served me well when it went live in 2021. However, I started my bachelor’s in Business Analytics program that year, and I started a new job on top of being a busy husband and father. Hence, the site has gone largely untouched since then. Earlier this year, however, I got the itch to rework my website. But what to do? I experimented with Hugo before settling on Quarto as my platform of choice. But why Quarto?"
  },
  {
    "objectID": "work/2024/04/26/garden/index.html#planning-for-a-garden",
    "href": "work/2024/04/26/garden/index.html#planning-for-a-garden",
    "title": "Welcome to My Digital Garden",
    "section": "",
    "text": "I first heard about “digital gardens” during Vicki Boykis’s rstudio::global 2021 keynote entitled “Your Public Garden.” During it, she discussed how it’s important to build a digital garden as a way to take control of your online experience and create a more positive space. This approach intrigued me, so I wanted to learn more about it. Maggie Appleton did an excellent job exploring how digital gardens serve as personal knowledge management systems, allowing individuals to cultivate their thoughts and ideas in a non-linear format. There are many benefits to planting and tending to a digital garden, such as fostering creativity, enabling serendipitous discovery, and facilitating knowledge sharing. I found this concept delightful, so I planted my own digital garden.\nBefore this, I had a website I built with Gatsby. I liked that website. The design was a fork of Simplefolio by Jacobo Martínez. I modified the theme’s colors to be more purplish-blue than sea green, changed the font, and added links to my social networks and data analysis projects. It took a while to finish the site, and I wish I documented the process. It served me well when it went live in 2021. However, I started my bachelor’s in Business Analytics program that year, and I started a new job on top of being a busy husband and father. Hence, the site has gone largely untouched since then. Earlier this year, however, I got the itch to rework my website. But what to do? I experimented with Hugo before settling on Quarto as my platform of choice. But why Quarto?"
  },
  {
    "objectID": "work/2024/04/26/garden/index.html#why-quarto",
    "href": "work/2024/04/26/garden/index.html#why-quarto",
    "title": "Welcome to My Digital Garden",
    "section": "Why Quarto?",
    "text": "Why Quarto?\nI’d love to jump into a list of well-thought-out reasons for using Quarto to plant the seeds of my digital garden like Silvia Canelón did. However, it really boiled down to inspiration, availability, and ease of use.\n\nInspiration: Upon searching for websites built using Quarto, I was inspired by the myriad of creatives using it to build not only websites but also communities, and I wanted to join the crowd.\nAvailability: Quarto was already available within RStudio Desktop, a tool I’ve used extensively since starting my bachelor’s program.\nEase of Use: Quarto required little more than a knowledge of RMarkdown to get started. Since I already possessed that, the barrier to entry was low for me."
  },
  {
    "objectID": "work/2024/04/26/garden/index.html#planting-a-garden",
    "href": "work/2024/04/26/garden/index.html#planting-a-garden",
    "title": "Welcome to My Digital Garden",
    "section": "Planting a Garden",
    "text": "Planting a Garden\nHopefully, I won’t regret this later. Still, I won’t detail how I planted my digital garden’s seeds. Instead, I’m going to list where I got my information and inspiration from:\n\nJadey Ryan: Intermediate guide to publish a Quarto website with GitHub & Netlify\n\nThis was my go-to resource while building my site. If you’re interested in using Quarto to plant a digital garden, I highly recommend you start here. I did most of what Jadey outlined, with a few exceptions:\n\nI don’t use Netlify for deployment. I use GitHub Pages.\nI didn’t use usethis.\nWhile I want my garden to be accessible to as many people as possible, I’m not using Lighthouse to audit the accessibility.\n\n\nMarvin Schmitt: Create Your Website with Quarto: Complete Tutorial and Template\n\nI picked up a few tips and tricks here, mainly on adding new pages and deploying with GitHub Pages.\n\nSilvia Canelón\n\nShe inspired me to go repo-diving to learn how things were done on other people’s sites, primarily how she built her contact form1.\n\n\n\nSamantha Csik: Adding a blog to your existing Quarto website\n\nI followed Sam’s guidance in adding a blog (i.e., this section of my garden) to my website.\n\nShe also helped me figure out how to add some pizzazz to my web pages.\n\n\nHilda\n\nHilda is a graphic novel series and an animated television show created by Luke Pearson. It features gorgeous artwork which has been described as “Scandinavia by way of Miyazaki.” Take a look for yourself and I think you’ll get it.\n\n\n\n\n\nAn entire day in Trolberg"
  },
  {
    "objectID": "work/2024/04/26/garden/index.html#welcome-to-my-digital-garden",
    "href": "work/2024/04/26/garden/index.html#welcome-to-my-digital-garden",
    "title": "Welcome to My Digital Garden",
    "section": "Welcome to My Digital Garden",
    "text": "Welcome to My Digital Garden\nSo, welcome to my digital garden! I’m not entirely sure what this place will become, but I don’t envision it being an obnoxiously active spot vying for your clicks. I want this to be a place where I can share thoughts and ideas and, hopefully, be part of something bigger than myself."
  },
  {
    "objectID": "work/2024/04/26/garden/index.html#footnotes",
    "href": "work/2024/04/26/garden/index.html#footnotes",
    "title": "Welcome to My Digital Garden",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSilvia Canelón went repo-diving while working on her contact form and learned from Tidy Tales’ About page. I should probably update my source to reflect that.↩︎"
  },
  {
    "objectID": "work/index.html",
    "href": "work/index.html",
    "title": "Work",
    "section": "",
    "text": "Calling the OpenAI API from an IDE\n\n\nA Student’s Guide to Secure Development\n\n\n\n\n\n\nOct 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nProjecting Elementary and Secondary School Enrollment\n\n\nA comprehensive methodology for projecting elementary and secondary school enrollment using statistical techniques\n\n\n\n\n\n\nJun 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to My Digital Garden\n\n\nA quiet space I can call my own\n\n\n\n\n\n\nApr 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoronavirus (COVID-19) Deaths and Infection Rate\n\n\nAn independent examination of Coronavirus (COVID-19) death, infection, and vaccination data\n\n\n\n\n\n\nJul 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiodiversity in National Parks\n\n\nAn interpretation of biodiversity data from the National Park Service, particularly around the various species observed in different national park locations\n\n\n\n\n\n\nJul 2, 2021\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "work/2021/07/21/covid-19/index.html",
    "href": "work/2021/07/21/covid-19/index.html",
    "title": "Coronavirus (COVID-19) Deaths and Infection Rate",
    "section": "",
    "text": "The coronavirus pandemic has affected nearly all aspects of our lives, sickened more than 186 million people worldwide, and more than 4 million people have died so far. In addition, it led to global lockdowns, a new era of frugality, and a third of Americans working from home. So, as it continues to feel like we could get back to normal-ish life soon in America, I decided to examine some of the data.\nAlex Freberg’s Data Analyst Portfolio Project series is the influence behind this activity. I made modifications to satisfy questions I wanted to answer and avoid simply copying his work. Alex’s YouTube channel is an inspirational and aspirational treasure trove of knowledge and ideas. If you’re interested in data analytics or another related field, I highly recommend that you check him out (if you haven’t already).\nYou can find the data behind this undertaking here.\nNote: The data extract occurred on July 11, 2021.\nThe code behind this project is on GitHub.\nYou may view the dashboard at Tableau Public."
  },
  {
    "objectID": "work/2021/07/21/covid-19/index.html#preparation",
    "href": "work/2021/07/21/covid-19/index.html#preparation",
    "title": "Coronavirus (COVID-19) Deaths and Infection Rate",
    "section": "Preparation",
    "text": "Preparation\nThis project utilized Visual Studio Code, Azure Data Studio, Tableau, and Azure SQL Database. Furthermore, I also used the Python pandas library to download the data and prepare it. After that, I would usually use SQL to create the necessary tables and import the data from the CSVs into them, but I couldn’t get that to work with Azure SQL Database. (The code I attempted to use to that end can be seen here and here.) Since that was the case, I enlisted the SQL Server Import extension for Azure Data Studio. Then, with the data preparation complete, I moved my attention to data querying with SQL.\n\n\n\nThe SQL Server flat-file import wizard for Azure Data Studio"
  },
  {
    "objectID": "work/2021/07/21/covid-19/index.html#analysis",
    "href": "work/2021/07/21/covid-19/index.html#analysis",
    "title": "Coronavirus (COVID-19) Deaths and Infection Rate",
    "section": "Analysis",
    "text": "Analysis\nFirst, I wanted to calculate the total cases, total deaths, and the number of totally vaccinated people. While the information for infections and deaths is in one table, vaccination data is in another table. Therefore, I joined the tables together on the location and date columns. Next, I totaled the new_cases and new_deaths columns as Total Cases and Total Deaths, respectively. Then, I used the most considerable number from the people_fully_vaccinated column of the vaccinated table as my People Fully Vaccinated total. Finally, I excluded world, continental, and non-nation-specific information from the global numbers.\n\n\n\nCOVID-19 totals from July 11, 2021\n\n\nNext, I began querying data to formulate the death count of six countries: the United States, Brazil, India, Russia, France, and China. I chose the first five countries where the death toll was most significant when I initiated this project. I also chose China due to COVID-19 allegedly originating there. Once I decided which countries to include in my query, I only needed to add the new_deaths.\n\n\n\nCOVID-19 deaths by country as of July 11, 2021\n\n\nIt seems like every COVID-19 dashboard has a map. So why should mine be any different? With that in mind, I mapped the infection rate by country. First, I took the highest number from the total_cases and divided that by the population. Then, I listed the results per country and fed the results of the new table into Tableau.\n\n\n\nThe darker the color, the higher the infection rate\n\n\nFinally, I wanted to examine the infection rate over time, separated by country. To do so, I used the same query that I did for the map visualization but added date as a criterion.\n\n\n\nThe United States had the highest infection rate as of July 11, 2021."
  },
  {
    "objectID": "work/2021/07/21/covid-19/index.html#conclusion",
    "href": "work/2021/07/21/covid-19/index.html#conclusion",
    "title": "Coronavirus (COVID-19) Deaths and Infection Rate",
    "section": "Conclusion",
    "text": "Conclusion\nWhile most of the world’s attention is currently laser-focused on getting vaccines to more people to stem the spread of the coronavirus, vaccines alone will not eliminate the escalation of COVID-19. As a result, scientists have significant pressure to find a cure to help those who contract the virus.\nDoctors have some medications they can use to treat the effects of COVID-19, but developing a drug that targets the virus itself is a complex and costly procedure. More than a year into the pandemic, only one antiviral treatment — remdesivir — is currently recommended for use in the US. Experts say remdesivir is not nearly effective enough, though.\nScientists are hopeful that new drugs designed to stop the virus’ deadly reproduction could reduce hospitalizations and deaths from COVID-19. In addition, the treatments offer hope and a contingency plan for unvaccinated individuals, particularly in low-income countries lagging far behind in the race to vaccinate."
  },
  {
    "objectID": "404/index.html",
    "href": "404/index.html",
    "title": "Page Not Found",
    "section": "",
    "text": "The page you requested cannot be found (perhaps it was moved or renamed). Who knows? But it’s not here.\n\n\n\n Back to top"
  }
]